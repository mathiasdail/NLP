{"cells":[{"cell_type":"code","source":["%sh\nls -la /home/finetuned_lm_eng/data"],"metadata":{"application/vnd.home.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6c7b7f75-f51f-424c-bc7a-a35bc5fcca9f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from argparse import ArgumentParser\nfrom pathlib import Path\nimport os\nimport torch\nimport logging\nimport json\nimport random\nimport numpy as np \nfrom collections import namedtuple\nfrom tempfile import TemporaryDirectory\n\nimport logging\nlogger = spark._jvm.org.apache.log4j\nlogging.getLogger(\"py4j\").setLevel(logging.ERROR)\nfrom torch.utils.data import DataLoader, Dataset, RandomSampler\nfrom torch.utils.data.distributed import DistributedSampler\nfrom tqdm import tqdm\n\nfrom pytorch_transformers import WEIGHTS_NAME, CONFIG_NAME\nfrom pytorch_transformers.modeling_bert import BertForPreTraining\nfrom pytorch_transformers.tokenization_bert import BertTokenizer\nfrom pytorch_transformers.optimization import AdamW, WarmupLinearSchedule\n\nInputFeatures = namedtuple(\"InputFeatures\", \"input_ids input_mask segment_ids lm_label_ids is_next\")\n\nlog_format = '%(asctime)-10s: %(message)s'\nlogging.basicConfig(level=logging.INFO, format=log_format)\n\n\ndef convert_example_to_features(example, tokenizer, max_seq_length):\n    tokens = example[\"tokens\"]\n    segment_ids = example[\"segment_ids\"]\n    is_random_next = example[\"is_random_next\"]\n    masked_lm_positions = example[\"masked_lm_positions\"]\n    masked_lm_labels = example[\"masked_lm_labels\"]\n\n    assert len(tokens) == len(segment_ids) <= max_seq_length  # The preprocessed data should be already truncated\n    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n    masked_label_ids = tokenizer.convert_tokens_to_ids(masked_lm_labels)\n\n    input_array = np.zeros(max_seq_length, dtype=np.int)\n    input_array[:len(input_ids)] = input_ids\n\n    mask_array = np.zeros(max_seq_length, dtype=np.bool)\n    mask_array[:len(input_ids)] = 1\n\n    segment_array = np.zeros(max_seq_length, dtype=np.bool)\n    segment_array[:len(segment_ids)] = segment_ids\n\n    lm_label_array = np.full(max_seq_length, dtype=np.int, fill_value=-1)\n    lm_label_array[masked_lm_positions] = masked_label_ids\n\n    features = InputFeatures(input_ids=input_array,\n                             input_mask=mask_array,\n                             segment_ids=segment_array,\n                             lm_label_ids=lm_label_array,\n                             is_next=is_random_next)\n    return features\n\n\nclass PregeneratedDataset(Dataset):\n    def __init__(self, training_path, epoch, tokenizer, num_data_epochs, reduce_memory=False):\n        self.vocab = tokenizer.vocab\n        self.tokenizer = tokenizer\n        self.epoch = epoch\n        self.data_epoch = epoch % num_data_epochs\n        data_file = training_path / f\"epoch_{self.data_epoch}.json\"\n        metrics_file = training_path / f\"epoch_{self.data_epoch}_metrics.json\"\n        assert data_file.is_file() and metrics_file.is_file()\n        metrics = json.loads(metrics_file.read_text())\n        num_samples = metrics['num_training_examples']\n        seq_len = metrics['max_seq_len']\n        self.temp_dir = None\n        self.working_dir = None\n        if reduce_memory:\n            self.temp_dir = TemporaryDirectory()\n            self.working_dir = Path(self.temp_dir.name)\n            input_ids = np.memmap(filename=self.working_dir/'input_ids.memmap',\n                                  mode='w+', dtype=np.int32, shape=(num_samples, seq_len))\n            input_masks = np.memmap(filename=self.working_dir/'input_masks.memmap',\n                                    shape=(num_samples, seq_len), mode='w+', dtype=np.bool)\n            segment_ids = np.memmap(filename=self.working_dir/'segment_ids.memmap',\n                                    shape=(num_samples, seq_len), mode='w+', dtype=np.bool)\n            lm_label_ids = np.memmap(filename=self.working_dir/'lm_label_ids.memmap',\n                                     shape=(num_samples, seq_len), mode='w+', dtype=np.int32)\n            lm_label_ids[:] = -1\n            is_nexts = np.memmap(filename=self.working_dir/'is_nexts.memmap',\n                                 shape=(num_samples,), mode='w+', dtype=np.bool)\n        else:\n            input_ids = np.zeros(shape=(num_samples, seq_len), dtype=np.int32)\n            input_masks = np.zeros(shape=(num_samples, seq_len), dtype=np.bool)\n            segment_ids = np.zeros(shape=(num_samples, seq_len), dtype=np.bool)\n            lm_label_ids = np.full(shape=(num_samples, seq_len), dtype=np.int32, fill_value=-1)\n            is_nexts = np.zeros(shape=(num_samples,), dtype=np.bool)\n        logging.info(f\"Loading training examples for epoch {epoch}\")\n        with data_file.open() as f:\n            for i, line in enumerate(tqdm(f, total=num_samples, desc=\"Training examples\")):\n                line = line.strip()\n                example = json.loads(line)\n                features = convert_example_to_features(example, tokenizer, seq_len)\n                input_ids[i] = features.input_ids\n                segment_ids[i] = features.segment_ids\n                input_masks[i] = features.input_mask\n                lm_label_ids[i] = features.lm_label_ids\n                is_nexts[i] = features.is_next\n        assert i == num_samples - 1  # Assert that the sample count metric was true\n        logging.info(\"Loading complete!\")\n        self.num_samples = num_samples\n        self.seq_len = seq_len\n        self.input_ids = input_ids\n        self.input_masks = input_masks\n        self.segment_ids = segment_ids\n        self.lm_label_ids = lm_label_ids\n        self.is_nexts = is_nexts\n\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, item):\n        return (torch.tensor(self.input_ids[item].astype(np.int64)),\n                torch.tensor(self.input_masks[item].astype(np.int64)),\n                torch.tensor(self.segment_ids[item].astype(np.int64)),\n                torch.tensor(self.lm_label_ids[item].astype(np.int64)),\n                torch.tensor(self.is_nexts[item].astype(np.int64)))\n\n\ndef main():\n    parser = ArgumentParser()\n    parser.add_argument('--pregenerated_data', type=Path, required=True)\n    parser.add_argument('--output_dir', type=Path, required=True)\n    parser.add_argument(\"--bert_model\", type=str, required=True, help=\"Bert pre-trained model selected in the list: bert-base-uncased, \"\n                             \"bert-large-uncased, bert-base-cased, bert-base-multilingual, bert-base-chinese.\")\n    \n    \n    #pregenerated_data_path =  'finetuned_lm_3e/data/'\n    pregenerated_data_path =  'finetuned_lm_eng/data/'\n    pregenerated_data = Path(pregenerated_data_path)\n    #train_corpus = '/tables/corpus.txt'\n    output_dir_path = '/home/finetune_lm/model_eng/'\n    output_dir = Path(output_dir_path)\n    #bert_model = 'bert-base-multilingual-cased'\n    bert_model = 'bert-base-cased'\n    \n    do_lower_case = False\n    reduce_memory = False\n    epochs = 3\n    local_rank = -1\n    no_cuda = True\n    gradient_accumulation_steps = 1\n    train_batch_size = 32\n    fp16 = False\n    loss_scale = 0.0\n    warmup_steps = 0\n    adam_epsilon = 1e-8\n    learning_rate = 3e-5\n    seed = 42\n\n    assert pregenerated_data.is_dir(), \\\n        \"--pregenerated_data should point to the folder of files made by pregenerate_training_data.py!\"\n\n    samples_per_epoch = []\n    for i in range(epochs):\n        epoch_file = pregenerated_data / f\"epoch_{i}.json\"\n        metrics_file = pregenerated_data / f\"epoch_{i}_metrics.json\"\n        if epoch_file.is_file() and metrics_file.is_file():\n            metrics = json.loads(metrics_file.read_text())\n            samples_per_epoch.append(metrics['num_training_examples'])\n        else:\n            if i == 0:\n                exit(\"No training data was found!\")\n            print(f\"Warning! There are fewer epochs of pregenerated data ({i}) than training epochs ({epochs}).\")\n            print(\"This script will loop over the available data, but training diversity may be negatively impacted.\")\n            num_data_epochs = i\n            break\n    else:\n        num_data_epochs = epochs\n\n    if local_rank == -1 or no_cuda:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() and not no_cuda else \"cpu\")\n        n_gpu = torch.cuda.device_count()\n    else:\n        torch.cuda.set_device(local_rank)\n        device = torch.device(\"cuda\", local_rank)\n        n_gpu = 1\n        # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n        torch.distributed.init_process_group(backend='nccl')\n    logging.info(\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\".format(\n        device, n_gpu, bool(local_rank != -1), fp16))\n\n    if gradient_accumulation_steps < 1:\n        raise ValueError(\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\".format(\n                            gradient_accumulation_steps))\n\n    train_batch_size = train_batch_size // gradient_accumulation_steps\n\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if n_gpu > 0:\n        torch.cuda.manual_seed_all(seed)\n\n    if output_dir.is_dir() and list(output_dir.iterdir()):\n        logging.warning(f\"Output directory ({output_dir}) already exists and is not empty!\")\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    tokenizer = BertTokenizer.from_pretrained(bert_model, do_lower_case=do_lower_case)\n\n    total_train_examples = 0\n    for i in range(epochs):\n        # The modulo takes into account the fact that we may loop over limited epochs of data\n        total_train_examples += samples_per_epoch[i % len(samples_per_epoch)]\n\n    num_train_optimization_steps = int(\n        total_train_examples / train_batch_size / gradient_accumulation_steps)\n    if local_rank != -1:\n        num_train_optimization_steps = num_train_optimization_steps // torch.distributed.get_world_size()\n\n    # Prepare model\n    model = BertForPreTraining.from_pretrained(bert_model)\n    # We don't need to manually call model.half() following Apex's recommend\n    # if fp16:\n    #     model.half()\n    model.to(device)\n    if local_rank != -1:\n        try:\n            from apex.parallel import DistributedDataParallel as DDP\n        except ImportError:\n            raise ImportError(\n                \"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n        model = DDP(model)\n    elif n_gpu > 1:\n        model = torch.nn.DataParallel(model)\n\n    # Prepare optimizer\n    param_optimizer = list(model.named_parameters())\n    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n         'weight_decay': 0.01},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n    ]\n\n    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)\n    scheduler = WarmupLinearSchedule(optimizer, warmup_steps=warmup_steps,\n                                     t_total=num_train_optimization_steps)\n\n    if fp16:\n        try:\n            # from apex.optimizers import FP16_Optimizer\n            # from apex.optimizers import FusedAdam\n            from apex import amp\n        except ImportError:\n            raise ImportError(\n                \"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n\n        # This below line of code is the main upgrade of Apex Fp16 implementation. I chose opt_leve=\"01\"\n        # because it's recommended for typical use by Apex. We can make it configured\n        model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\n\n    # We don't need to use FP16_Optimizer wrapping over FusedAdam as well. Now Apex supports all Pytorch Optimizer\n\n    #     optimizer = FusedAdam(optimizer_grouped_parameters,\n    #                           lr=learning_rate,\n    #                           bias_correction=False,\n    #                           max_grad_norm=1.0)\n    #     if loss_scale == 0:\n    #         optimizer = FP16_Optimizer(optimizer, dynamic_loss_scale=True)\n    #     else:\n    #         optimizer = FP16_Optimizer(optimizer, static_loss_scale=loss_scale)\n    # else:\n    #     optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)\n    # scheduler = WarmupLinearSchedule(optimizer, warmup_steps=warmup_steps, t_total=num_train_optimization_steps)\n\n    global_step = 0\n    logging.info(\"***** Running training *****\")\n    logging.info(f\"  Num examples = {total_train_examples}\")\n    logging.info(\"  Batch size = %d\", train_batch_size)\n    logging.info(\"  Num steps = %d\", num_train_optimization_steps)\n    model.train()\n    for epoch in range(epochs):\n        epoch_dataset = PregeneratedDataset(epoch=epoch, training_path=pregenerated_data, tokenizer=tokenizer,\n                                            num_data_epochs=num_data_epochs, reduce_memory=reduce_memory)\n        if local_rank == -1:\n            train_sampler = RandomSampler(epoch_dataset)\n        else:\n            train_sampler = DistributedSampler(epoch_dataset)\n        train_dataloader = DataLoader(epoch_dataset, sampler=train_sampler, batch_size=train_batch_size)\n        tr_loss = 0\n        nb_tr_examples, nb_tr_steps = 0, 0\n        with tqdm(total=len(train_dataloader), desc=f\"Epoch {epoch}\") as pbar:\n            for step, batch in enumerate(train_dataloader):\n                batch = tuple(t.to(device) for t in batch)\n                input_ids, input_mask, segment_ids, lm_label_ids, is_next = batch\n                outputs = model(input_ids, segment_ids, input_mask, lm_label_ids, is_next)\n                loss = outputs[0]\n                if n_gpu > 1:\n                    loss = loss.mean() # mean() to average on multi-gpu.\n                if gradient_accumulation_steps > 1:\n                    loss = loss / gradient_accumulation_steps\n                if fp16:\n                    # I depricate FP16_Optimizer's backward func and replace as Apex document\n                    # optimizer.backward(loss)\n                    with amp.scale_loss(loss, optimizer) as scaled_loss:\n                        scaled_loss.backward()\n                else:\n                    loss.backward()\n                tr_loss += loss.item()\n                nb_tr_examples += input_ids.size(0)\n                nb_tr_steps += 1\n                pbar.update(1)\n                mean_loss = tr_loss * gradient_accumulation_steps / nb_tr_steps\n                pbar.set_postfix_str(f\"Loss: {mean_loss:.5f}\")\n                if (step + 1) % gradient_accumulation_steps == 0:\n                    optimizer.step()\n                    scheduler.step()  # Update learning rate schedule\n                    optimizer.zero_grad()\n                    global_step += 1\n\n    # Save a trained model\n    if local_rank == -1 or torch.distributed.get_rank() == 0:\n        logging.info(\"** ** * Saving fine-tuned model ** ** * \")\n        model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n        model_to_save.save_pretrained(output_dir)\n        tokenizer.save_pretrained(output_dir)\n\n\nif __name__ == '__main__':\n    main()"],"metadata":{"application/vnd.home.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f079812b-282a-49a6-abf6-3d644d3f528a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.home.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">2019-09-13 09:17:28,494: device: cpu n_gpu: 0, distributed training: False, 16-bits training: False\n2019-09-13 09:17:29,003: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt not found in cache, downloading to /tmp/tmp1l4zc8ab\n\r  0%|          | 0/213450 [00:00&lt;?, ?B/s]\r  8%|▊         | 17408/213450 [00:00&lt;00:01, 105573.23B/s]\r 24%|██▍       | 52224/213450 [00:00&lt;00:01, 123878.00B/s]\r 49%|████▉     | 104448/213450 [00:00&lt;00:00, 151197.16B/s]\r 82%|████████▏ | 174080/213450 [00:00&lt;00:00, 186890.14B/s]\r100%|██████████| 213450/213450 [00:00&lt;00:00, 318858.23B/s]\n2019-09-13 09:17:30,044: copying /tmp/tmp1l4zc8ab to cache at /root/.cache/torch/pytorch_transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n2019-09-13 09:17:30,045: creating metadata file for /root/.cache/torch/pytorch_transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n2019-09-13 09:17:30,046: removing temp file /tmp/tmp1l4zc8ab\n2019-09-13 09:17:30,046: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /root/.cache/torch/pytorch_transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n2019-09-13 09:17:30,508: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json not found in cache, downloading to /tmp/tmp64tzlt8_\n\r  0%|          | 0/313 [00:00&lt;?, ?B/s]\r100%|██████████| 313/313 [00:00&lt;00:00, 251835.25B/s]\n2019-09-13 09:17:30,903: copying /tmp/tmp64tzlt8_ to cache at /root/.cache/torch/pytorch_transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.d7a3af18ce3a2ab7c0f48f04dc8daff45ed9a3ed333b9e9a79d012a0dedf87a6\n2019-09-13 09:17:30,903: creating metadata file for /root/.cache/torch/pytorch_transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.d7a3af18ce3a2ab7c0f48f04dc8daff45ed9a3ed333b9e9a79d012a0dedf87a6\n2019-09-13 09:17:30,903: removing temp file /tmp/tmp64tzlt8_\n2019-09-13 09:17:30,904: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /root/.cache/torch/pytorch_transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.d7a3af18ce3a2ab7c0f48f04dc8daff45ed9a3ed333b9e9a79d012a0dedf87a6\n2019-09-13 09:17:30,904: Model config {\n  &#34;attention_probs_dropout_prob&#34;: 0.1,\n  &#34;finetuning_task&#34;: null,\n  &#34;hidden_act&#34;: &#34;gelu&#34;,\n  &#34;hidden_dropout_prob&#34;: 0.1,\n  &#34;hidden_size&#34;: 768,\n  &#34;initializer_range&#34;: 0.02,\n  &#34;intermediate_size&#34;: 3072,\n  &#34;layer_norm_eps&#34;: 1e-12,\n  &#34;max_position_embeddings&#34;: 512,\n  &#34;num_attention_heads&#34;: 12,\n  &#34;num_hidden_layers&#34;: 12,\n  &#34;num_labels&#34;: 2,\n  &#34;output_attentions&#34;: false,\n  &#34;output_hidden_states&#34;: false,\n  &#34;torchscript&#34;: false,\n  &#34;type_vocab_size&#34;: 2,\n  &#34;vocab_size&#34;: 28996\n}\n\n2019-09-13 09:17:31,300: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin not found in cache, downloading to /tmp/tmpe8rv61u3\n\r  0%|          | 0/435779157 [00:00&lt;?, ?B/s]\r  0%|          | 52224/435779157 [00:00&lt;22:59, 315795.49B/s]\r  0%|          | 278528/435779157 [00:00&lt;17:41, 410271.85B/s]\r  0%|          | 957440/435779157 [00:00&lt;12:53, 561864.09B/s]\r  1%|          | 2353152/435779157 [00:00&lt;09:09, 789046.61B/s]\r  1%|          | 4104192/435779157 [00:00&lt;06:34, 1095466.28B/s]\r  2%|▏         | 6861824/435779157 [00:00&lt;04:38, 1538752.24B/s]\r  2%|▏         | 9433088/435779157 [00:00&lt;03:18, 2143248.42B/s]\r  3%|▎         | 11591680/435779157 [00:01&lt;02:26, 2897650.70B/s]\r  3%|▎         | 14273536/435779157 [00:01&lt;01:46, 3956297.94B/s]\r  4%|▍         | 16934912/435779157 [00:01&lt;01:18, 5313314.66B/s]\r  4%|▍         | 19119104/435779157 [00:01&lt;01:00, 6859928.93B/s]\r  5%|▌         | 22667264/435779157 [00:01&lt;00:46, 8953528.18B/s]\r  6%|▌         | 26138624/435779157 [00:01&lt;00:35, 11517550.03B/s]\r  7%|▋         | 28897280/435779157 [00:01&lt;00:29, 13663832.32B/s]\r  7%|▋         | 32110592/435779157 [00:01&lt;00:24, 16507012.82B/s]\r  8%|▊         | 34988032/435779157 [00:01&lt;00:21, 18849198.79B/s]\r  9%|▊         | 38122496/435779157 [00:02&lt;00:18, 21409514.88B/s]\r  9%|▉         | 41037824/435779157 [00:02&lt;00:17, 22411876.92B/s]\r 10%|█         | 44226560/435779157 [00:02&lt;00:15, 24604751.49B/s]\r 11%|█         | 47139840/435779157 [00:02&lt;00:15, 25297504.91B/s]\r 12%|█▏        | 50487296/435779157 [00:02&lt;00:14, 25819546.81B/s]\r 12%|█▏        | 53689344/435779157 [00:02&lt;00:13, 27411548.27B/s]\r 13%|█▎        | 56616960/435779157 [00:02&lt;00:13, 27479902.82B/s]\r 14%|█▎        | 59875328/435779157 [00:02&lt;00:13, 28831854.59B/s]\r 14%|█▍        | 62867456/435779157 [00:02&lt;00:13, 27522540.62B/s]\r 15%|█▌        | 65934336/435779157 [00:02&lt;00:13, 28396435.66B/s]\r 16%|█▌        | 68843520/435779157 [00:03&lt;00:12, 28343460.41B/s]\r 17%|█▋        | 71983104/435779157 [00:03&lt;00:13, 27674290.43B/s]\r 17%|█▋        | 75143168/435779157 [00:03&lt;00:12, 28743301.44B/s]\r 18%|█▊        | 78061568/435779157 [00:03&lt;00:12, 28673658.26B/s]\r 19%|█▊        | 81205248/435779157 [00:03&lt;00:12, 29449120.37B/s]\r 19%|█▉        | 84173824/435779157 [00:03&lt;00:12, 27945343.97B/s]\r 20%|██        | 87469056/435779157 [00:03&lt;00:11, 29280029.05B/s]\r 21%|██        | 90436608/435779157 [00:03&lt;00:11, 28791837.44B/s]\r 21%|██▏       | 93577216/435779157 [00:03&lt;00:12, 27888884.18B/s]\r 22%|██▏       | 96824320/435779157 [00:04&lt;00:11, 29121541.81B/s]\r 23%|██▎       | 99770368/435779157 [00:04&lt;00:11, 28702943.48B/s]\r 24%|██▎       | 103172096/435779157 [00:04&lt;00:11, 30113320.10B/s]\r 24%|██▍       | 106219520/435779157 [00:04&lt;00:11, 28420217.95B/s]\r 25%|██▌       | 109305856/435779157 [00:04&lt;00:11, 28446348.86B/s]\r 26%|██▌       | 112278528/435779157 [00:04&lt;00:11, 28817616.63B/s]\r 26%|██▋       | 115204096/435779157 [00:04&lt;00:11, 27921790.06B/s]\r 27%|██▋       | 118458368/435779157 [00:04&lt;00:10, 29164041.35B/s]\r 28%|██▊       | 121405440/435779157 [00:04&lt;00:10, 28692028.92B/s]\r 29%|██▊       | 124657664/435779157 [00:05&lt;00:10, 29738952.23B/s]\r 29%|██▉       | 127657984/435779157 [00:05&lt;00:10, 28121043.77B/s]\r 30%|███       | 130883584/435779157 [00:05&lt;00:10, 28490967.48B/s]\r 31%|███       | 134105088/435779157 [00:05&lt;00:10, 29513305.36B/s]\r 31%|███▏      | 137084928/435779157 [00:05&lt;00:10, 28157785.46B/s]\r 32%|███▏      | 140255232/435779157 [00:05&lt;00:10, 29131911.12B/s]\r 33%|███▎      | 143200256/435779157 [00:05&lt;00:10, 28639428.30B/s]\r 34%|███▎      | 146497536/435779157 [00:05&lt;00:10, 28143942.55B/s]\r 34%|███▍      | 149819392/435779157 [00:05&lt;00:09, 29495344.91B/s]\r 35%|███▌      | 152799232/435779157 [00:05&lt;00:09, 28859319.62B/s]\r 36%|███▌      | 155987968/435779157 [00:06&lt;00:09, 29704773.17B/s]\r 36%|███▋      | 158982144/435779157 [00:06&lt;00:09, 28388630.54B/s]\r 37%|███▋      | 162213888/435779157 [00:06&lt;00:09, 29462549.78B/s]\r 38%|███▊      | 165190656/435779157 [00:06&lt;00:09, 28792607.08B/s]\r 39%|███▊      | 168271872/435779157 [00:06&lt;00:09, 28129839.77B/s]\r 39%|███▉      | 171499520/435779157 [00:06&lt;00:09, 29256021.10B/s]\r 40%|████      | 174451712/435779157 [00:06&lt;00:09, 28704747.34B/s]\r 41%|████      | 177831936/435779157 [00:06&lt;00:08, 30064042.59B/s]\r 42%|████▏     | 180870144/435779157 [00:06&lt;00:08, 28456602.57B/s]\r 42%|████▏     | 184016896/435779157 [00:07&lt;00:08, 28535677.96B/s]\r 43%|████▎     | 187234304/435779157 [00:07&lt;00:08, 29537127.90B/s]\r 44%|████▎     | 190217216/435779157 [00:07&lt;00:08, 28022287.58B/s]\r 44%|████▍     | 193501184/435779157 [00:07&lt;00:08, 29311697.44B/s]\r 45%|████▌     | 196473856/435779157 [00:07&lt;00:08, 28579443.91B/s]\r 46%|████▌     | 199483392/435779157 [00:07&lt;00:08, 27760354.40B/s]\r 47%|████▋     | 202702848/435779157 [00:07&lt;00:08, 28955221.90B/s]\r 47%|████▋     | 205631488/435779157 [00:07&lt;00:08, 28373496.33B/s]\r 48%|████▊     | 208945152/435779157 [00:07&lt;00:07, 29650953.95B/s]\r 49%|████▊     | 211943424/435779157 [00:08&lt;00:07, 28213472.04B/s]\r 49%|████▉     | 215099392/435779157 [00:08&lt;00:07, 29139714.56B/s]\r 50%|█████     | 218047488/435779157 [00:08&lt;00:07, 28870076.14B/s]\r 51%|█████     | 221143040/435779157 [00:08&lt;00:07, 27990858.89B/s]\r 51%|█████▏    | 224292864/435779157 [00:08&lt;00:07, 28957601.92B/s]\r 52%|█████▏    | 227213312/435779157 [00:08&lt;00:07, 28475013.21B/s]\r 53%|█████▎    | 230514688/435779157 [00:08&lt;00:06, 29699454.19B/s]\r 54%|█████▎    | 233511936/435779157 [00:08&lt;00:07, 28049681.68B/s]\r 54%|█████▍    | 236724224/435779157 [00:08&lt;00:07, 28361428.38B/s]\r 55%|█████▌    | 239837184/435779157 [00:09&lt;00:06, 29136749.06B/s]\r 56%|█████▌    | 242776064/435779157 [00:09&lt;00:06, 27960027.94B/s]\r 56%|█████▋    | 246210560/435779157 [00:09&lt;00:06, 29611266.54B/s]\r 57%|█████▋    | 249218048/435779157 [00:09&lt;00:06, 28864609.10B/s]\r 58%|█████▊    | 252361728/435779157 [00:09&lt;00:06, 29590288.39B/s]\r 59%|█████▊    | 255350784/435779157 [00:09&lt;00:06, 28276704.17B/s]\r 59%|█████▉    | 258433024/435779157 [00:09&lt;00:06, 28419473.67B/s]\r 60%|██████    | 261579776/435779157 [00:09&lt;00:05, 29269512.80B/s]\r 61%|██████    | 264529920/435779157 [00:09&lt;00:06, 28218278.71B/s]\r 61%|██████▏   | 267907072/435779157 [00:09&lt;00:05, 29681967.85B/s]\r 62%|██████▏   | 270912512/435779157 [00:10&lt;00:05, 28900680.87B/s]\r 63%|██████▎   | 273955840/435779157 [00:10&lt;00:05, 29343991.91B/s]\r 64%|██████▎   | 276914176/435779157 [00:10&lt;00:05, 28415191.59B/s]\r 64%|██████▍   | 280289280/435779157 [00:10&lt;00:05, 28757125.75B/s]\r 65%|██████▌   | 283557888/435779157 [00:10&lt;00:05, 29831647.42B/s]\r 66%|██████▌   | 286564352/435779157 [00:10&lt;00:05, 28843832.54B/s]\r 66%|██████▋   | 289759232/435779157 [00:10&lt;00:04, 29707173.47B/s]\r 67%|██████▋   | 292752384/435779157 [00:10&lt;00:04, 28869866.21B/s]\r 68%|██████▊   | 295991296/435779157 [00:10&lt;00:04, 29842092.74B/s]\r 69%|██████▊   | 298998784/435779157 [00:11&lt;00:04, 28622469.59B/s]\r 69%|██████▉   | 302227456/435779157 [00:11&lt;00:04, 28728522.62B/s]\r 70%|███████   | 305463296/435779157 [00:11&lt;00:04, 29728888.68B/s]\r 71%|███████   | 308458496/435779157 [00:11&lt;00:04, 28678496.41B/s]\r 72%|███████▏  | 311633920/435779157 [00:11&lt;00:04, 29535986.26B/s]\r 72%|███████▏  | 314610688/435779157 [00:11&lt;00:04, 28716586.54B/s]\r 73%|███████▎  | 317988864/435779157 [00:11&lt;00:03, 30065108.51B/s]\r 74%|███████▎  | 321027072/435779157 [00:11&lt;00:03, 28867403.43B/s]\r 74%|███████▍  | 324116480/435779157 [00:11&lt;00:03, 28596606.06B/s]\r 75%|███████▌  | 327314432/435779157 [00:12&lt;00:03, 29533484.77B/s]\r 76%|███████▌  | 330375168/435779157 [00:12&lt;00:03, 28747032.83B/s]\r 77%|███████▋  | 333547520/435779157 [00:12&lt;00:03, 29578753.56B/s]\r 77%|███████▋  | 336526336/435779157 [00:12&lt;00:03, 28661618.20B/s]\r 78%|███████▊  | 339872768/435779157 [00:12&lt;00:03, 29950659.59B/s]\r 79%|███████▊  | 342897664/435779157 [00:12&lt;00:03, 28801414.84B/s]\r 79%|███████▉  | 345972736/435779157 [00:12&lt;00:03, 28538400.79B/s]\r 80%|████████  | 349150208/435779157 [00:12&lt;00:02, 29437667.12B/s]\r 81%|████████  | 352165888/435779157 [00:12&lt;00:02, 28537186.00B/s]\r 82%|████████▏ | 355340288/435779157 [00:12&lt;00:02, 29428261.49B/s]\r 82%|████████▏ | 358305792/435779157 [00:13&lt;00:02, 28671772.39B/s]\r 83%|████████▎ | 361553920/435779157 [00:13&lt;00:02, 29716840.15B/s]\r 84%|████████▎ | 364550144/435779157 [00:13&lt;00:02, 28637603.01B/s]\r 84%|████████▍ | 367812608/435779157 [00:13&lt;00:02, 28725996.82B/s]\r 85%|████████▌ | 371021824/435779157 [00:13&lt;00:02, 29659023.65B/s]\r 86%|████████▌ | 374022144/435779157 [00:13&lt;00:02, 28641700.76B/s]\r 87%|████████▋ | 377267200/435779157 [00:13&lt;00:01, 29686765.43B/s]\r 87%|████████▋ | 380261376/435779157 [00:13&lt;00:01, 29078860.62B/s]\r 88%|████████▊ | 383404032/435779157 [00:13&lt;00:01, 29744819.00B/s]\r 89%|████████▊ | 386397184/435779157 [00:14&lt;00:01, 28425263.46B/s]\r 89%|████████▉ | 389472256/435779157 [00:14&lt;00:01, 29083389.49B/s]\r 90%|█████████ | 392401920/435779157 [00:14&lt;00:01, 28535511.65B/s]\r 91%|█████████ | 395747328/435779157 [00:14&lt;00:01, 28496536.24B/s]\r 92%|█████████▏| 399072256/435779157 [00:14&lt;00:01, 29773154.58B/s]\r 92%|█████████▏| 402073600/435779157 [00:14&lt;00:01, 28864755.49B/s]\r 93%|█████████▎| 405512192/435779157 [00:14&lt;00:01, 28656392.08B/s]\r 94%|█████████▍| 408914944/435779157 [00:14&lt;00:00, 30080257.43B/s]\r 95%|█████████▍| 411953152/435779157 [00:14&lt;00:00, 29166430.85B/s]\r 95%|█████████▌| 415149056/435779157 [00:15&lt;00:00, 29950694.34B/s]\r 96%|█████████▌| 418168832/435779157 [00:15&lt;00:00, 28853098.06B/s]\r 97%|█████████▋| 421339136/435779157 [00:15&lt;00:00, 28790713.20B/s]\r 97%|█████████▋| 424400896/435779157 [00:15&lt;00:00, 29314474.91B/s]\r 98%|█████████▊| 427433984/435779157 [00:15&lt;00:00, 28506446.08B/s]\r 99%|█████████▉| 430580736/435779157 [00:15&lt;00:00, 29333897.88B/s]\r 99%|█████████▉| 433530880/435779157 [00:15&lt;00:00, 28677995.43B/s]\r100%|██████████| 435779157/435779157 [00:15&lt;00:00, 27718892.14B/s]\n2019-09-13 09:17:47,488: copying /tmp/tmpe8rv61u3 to cache at /root/.cache/torch/pytorch_transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\n2019-09-13 09:17:48,057: creating metadata file for /root/.cache/torch/pytorch_transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\n2019-09-13 09:17:48,057: removing temp file /tmp/tmpe8rv61u3\n2019-09-13 09:17:48,136: loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /root/.cache/torch/pytorch_transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\n2019-09-13 09:17:52,567: ***** Running training *****\n2019-09-13 09:17:52,567:   Num examples = 1468\n2019-09-13 09:17:52,568:   Batch size = 32\n2019-09-13 09:17:52,568:   Num steps = 45\n2019-09-13 09:17:52,570: Loading training examples for epoch 0\n\rTraining examples:   0%|          | 0/490 [00:00&lt;?, ?it/s]\rTraining examples: 100%|██████████| 490/490 [00:00&lt;00:00, 7828.17it/s]\n2019-09-13 09:17:52,633: Loading complete!\n\rEpoch 0:   0%|          | 0/16 [00:00&lt;?, ?it/s]\rEpoch 0:   6%|▋         | 1/16 [00:10&lt;02:34, 10.32s/it]\rEpoch 0:   6%|▋         | 1/16 [00:10&lt;02:34, 10.32s/it, Loss: 6.93127]\rEpoch 0:  12%|█▎        | 2/16 [00:21&lt;02:26, 10.43s/it, Loss: 6.93127]\rEpoch 0:  12%|█▎        | 2/16 [00:21&lt;02:26, 10.43s/it, Loss: 6.02028]\rEpoch 0:  19%|█▉        | 3/16 [00:31&lt;02:16, 10.48s/it, Loss: 6.02028]\rEpoch 0:  19%|█▉        | 3/16 [00:31&lt;02:16, 10.48s/it, Loss: 5.55043]\rEpoch 0:  25%|██▌       | 4/16 [00:42&lt;02:06, 10.52s/it, Loss: 5.55043]\rEpoch 0:  25%|██▌       | 4/16 [00:42&lt;02:06, 10.52s/it, Loss: 5.17909]\rEpoch 0:  31%|███▏      | 5/16 [00:52&lt;01:55, 10.47s/it, Loss: 5.17909]\rEpoch 0:  31%|███▏      | 5/16 [00:52&lt;01:55, 10.47s/it, Loss: 4.78345]\rEpoch 0:  38%|███▊      | 6/16 [01:03&lt;01:45, 10.52s/it, Loss: 4.78345]\rEpoch 0:  38%|███▊      | 6/16 [01:03&lt;01:45, 10.52s/it, Loss: 4.58648]\rEpoch 0:  44%|████▍     | 7/16 [01:13&lt;01:34, 10.45s/it, Loss: 4.58648]\rEpoch 0:  44%|████▍     | 7/16 [01:13&lt;01:34, 10.45s/it, Loss: 4.41470]\rEpoch 0:  50%|█████     | 8/16 [01:23&lt;01:23, 10.42s/it, Loss: 4.41470]\rEpoch 0:  50%|█████     | 8/16 [01:23&lt;01:23, 10.42s/it, Loss: 4.25623]\rEpoch 0:  56%|█████▋    | 9/16 [01:34&lt;01:12, 10.39s/it, Loss: 4.25623]\rEpoch 0:  56%|█████▋    | 9/16 [01:34&lt;01:12, 10.39s/it, Loss: 4.17582]\rEpoch 0:  62%|██████▎   | 10/16 [01:44&lt;01:02, 10.37s/it, Loss: 4.17582]\rEpoch 0:  62%|██████▎   | 10/16 [01:44&lt;01:02, 10.37s/it, Loss: 4.07387]\rEpoch 0:  69%|██████▉   | 11/16 [01:54&lt;00:51, 10.37s/it, Loss: 4.07387]\rEpoch 0:  69%|██████▉   | 11/16 [01:54&lt;00:51, 10.37s/it, Loss: 4.00849]\rEpoch 0:  75%|███████▌  | 12/16 [02:05&lt;00:41, 10.36s/it, Loss: 4.00849]\rEpoch 0:  75%|███████▌  | 12/16 [02:05&lt;00:41, 10.36s/it, Loss: 3.95066]\rEpoch 0:  81%|████████▏ | 13/16 [02:15&lt;00:31, 10.34s/it, Loss: 3.95066]\rEpoch 0:  81%|████████▏ | 13/16 [02:15&lt;00:31, 10.34s/it, Loss: 3.92277]\rEpoch 0:  88%|████████▊ | 14/16 [02:25&lt;00:20, 10.38s/it, Loss: 3.92277]\rEpoch 0:  88%|████████▊ | 14/16 [02:25&lt;00:20, 10.38s/it, Loss: 3.85591]\rEpoch 0:  94%|█████████▍| 15/16 [02:36&lt;00:10, 10.48s/it, Loss: 3.85591]\rEpoch 0:  94%|█████████▍| 15/16 [02:36&lt;00:10, 10.48s/it, Loss: 3.81644]\rEpoch 0: 100%|██████████| 16/16 [02:40&lt;00:00,  8.35s/it, Loss: 3.81644]\rEpoch 0: 100%|██████████| 16/16 [02:40&lt;00:00,  8.35s/it, Loss: 3.81515]\n2019-09-13 09:20:33,074: Loading training examples for epoch 1\n\rTraining examples:   0%|          | 0/489 [00:00&lt;?, ?it/s]\rTraining examples: 100%|██████████| 489/489 [00:00&lt;00:00, 7557.64it/s]\n2019-09-13 09:20:33,140: Loading complete!\n\rEpoch 1:   0%|          | 0/16 [00:00&lt;?, ?it/s]\rEpoch 1:   6%|▋         | 1/16 [00:10&lt;02:30, 10.00s/it]\rEpoch 1:   6%|▋         | 1/16 [00:10&lt;02:30, 10.00s/it, Loss: 3.32886]\rEpoch 1:  12%|█▎        | 2/16 [00:20&lt;02:22, 10.21s/it, Loss: 3.32886]\rEpoch 1:  12%|█▎        | 2/16 [00:20&lt;02:22, 10.21s/it, Loss: 3.27360]\rEpoch 1:  19%|█▉        | 3/16 [00:31&lt;02:13, 10.27s/it, Loss: 3.27360]\rEpoch 1:  19%|█▉        | 3/16 [00:31&lt;02:13, 10.27s/it, Loss: 3.03251]\rEpoch 1:  25%|██▌       | 4/16 [00:41&lt;02:03, 10.30s/it, Loss: 3.03251]\rEpoch 1:  25%|██▌       | 4/16 [00:41&lt;02:03, 10.30s/it, Loss: 2.94789]\rEpoch 1:  31%|███▏      | 5/16 [00:52&lt;01:54, 10.37s/it, Loss: 2.94789]\rEpoch 1:  31%|███▏      | 5/16 [00:52&lt;01:54, 10.37s/it, Loss: 2.92848]\rEpoch 1:  38%|███▊      | 6/16 [01:02&lt;01:43, 10.35s/it, Loss: 2.92848]\rEpoch 1:  38%|███▊      | 6/16 [01:02&lt;01:43, 10.35s/it, Loss: 2.94156]\rEpoch 1:  44%|████▍     | 7/16 [01:12&lt;01:33, 10.42s/it, Loss: 2.94156]\rEpoch 1:  44%|████▍     | 7/16 [01:12&lt;01:33, 10.42s/it, Loss: 2.94744]\rEpoch 1:  50%|█████     | 8/16 [01:23&lt;01:23, 10.39s/it, Loss: 2.94744]\rEpoch 1:  50%|█████     | 8/16 [01:23&lt;01:23, 10.39s/it, Loss: 2.92721]\rEpoch 1:  56%|█████▋    | 9/16 [01:33&lt;01:12, 10.37s/it, Loss: 2.92721]\rEpoch 1:  56%|█████▋    | 9/16 [01:33&lt;01:12, 10.37s/it, Loss: 2.90332]\rEpoch 1:  62%|██████▎   | 10/16 [01:43&lt;01:02, 10.39s/it, Loss: 2.90332]\rEpoch 1:  62%|██████▎   | 10/16 [01:43&lt;01:02, 10.39s/it, Loss: 2.92808]\rEpoch 1:  69%|██████▉   | 11/16 [01:54&lt;00:52, 10.42s/it, Loss: 2.92808]\rEpoch 1:  69%|██████▉   | 11/16 [01:54&lt;00:52, 10.42s/it, Loss: 2.93523]\rEpoch 1:  75%|███████▌  | 12/16 [02:04&lt;00:41, 10.42s/it, Loss: 2.93523]\rEpoch 1:  75%|███████▌  | 12/16 [02:04&lt;00:41, 10.42s/it, Loss: 2.92545]\rEpoch 1:  81%|████████▏ | 13/16 [02:15&lt;00:31, 10.38s/it, Loss: 2.92545]\rEpoch 1:  81%|████████▏ | 13/16 [02:15&lt;00:31, 10.38s/it, Loss: 2.93394]\rEpoch 1:  88%|████████▊ | 14/16 [02:25&lt;00:20, 10.41s/it, Loss: 2.93394]\rEpoch 1:  88%|████████▊ | 14/16 [02:25&lt;00:20, 10.41s/it, Loss: 2.92805]\rEpoch 1:  94%|█████████▍| 15/16 [02:36&lt;00:10, 10.43s/it, Loss: 2.92805]\rEpoch 1:  94%|█████████▍| 15/16 [02:36&lt;00:10, 10.43s/it, Loss: 2.93455]\rEpoch 1: 100%|██████████| 16/16 [02:39&lt;00:00,  8.23s/it, Loss: 2.93455]\rEpoch 1: 100%|██████████| 16/16 [02:39&lt;00:00,  8.23s/it, Loss: 2.97023]\n2019-09-13 09:23:12,800: Loading training examples for epoch 2\n\rTraining examples:   0%|          | 0/489 [00:00&lt;?, ?it/s]\rTraining examples: 100%|██████████| 489/489 [00:00&lt;00:00, 7998.25it/s]\n2019-09-13 09:23:12,862: Loading complete!\n\rEpoch 2:   0%|          | 0/16 [00:00&lt;?, ?it/s]\rEpoch 2:   6%|▋         | 1/16 [00:10&lt;02:34, 10.30s/it]\rEpoch 2:   6%|▋         | 1/16 [00:10&lt;02:34, 10.30s/it, Loss: 2.56884]\rEpoch 2:  12%|█▎        | 2/16 [00:20&lt;02:24, 10.30s/it, Loss: 2.56884]\rEpoch 2:  12%|█▎        | 2/16 [00:20&lt;02:24, 10.30s/it, Loss: 2.49181]\rEpoch 2:  19%|█▉        | 3/16 [00:30&lt;02:14, 10.31s/it, Loss: 2.49181]\rEpoch 2:  19%|█▉        | 3/16 [00:30&lt;02:14, 10.31s/it, Loss: 2.40388]\rEpoch 2:  25%|██▌       | 4/16 [00:41&lt;02:04, 10.39s/it, Loss: 2.40388]\rEpoch 2:  25%|██▌       | 4/16 [00:41&lt;02:04, 10.39s/it, Loss: 2.50817]\rEpoch 2:  31%|███▏      | 5/16 [00:51&lt;01:54, 10.40s/it, Loss: 2.50817]\rEpoch 2:  31%|███▏      | 5/16 [00:51&lt;01:54, 10.40s/it, Loss: 2.58843]\rEpoch 2:  38%|███▊      | 6/16 [01:02&lt;01:43, 10.40s/it, Loss: 2.58843]\rEpoch 2:  38%|███▊      | 6/16 [01:02&lt;01:43, 10.40s/it, Loss: 2.60762]\rEpoch 2:  44%|████▍     | 7/16 [01:12&lt;01:33, 10.41s/it, Loss: 2.60762]\rEpoch 2:  44%|████▍     | 7/16 [01:12&lt;01:33, 10.41s/it, Loss: 2.66570]\rEpoch 2:  50%|█████     | 8/16 [01:23&lt;01:23, 10.39s/it, Loss: 2.66570]\rEpoch 2:  50%|█████     | 8/16 [01:23&lt;01:23, 10.39s/it, Loss: 2.69110]\rEpoch 2:  56%|█████▋    | 9/16 [01:33&lt;01:13, 10.44s/it, Loss: 2.69110]\rEpoch 2:  56%|█████▋    | 9/16 [01:33&lt;01:13, 10.44s/it, Loss: 2.70089]\rEpoch 2:  62%|██████▎   | 10/16 [01:44&lt;01:02, 10.41s/it, Loss: 2.70089]\rEpoch 2:  62%|██████▎   | 10/16 [01:44&lt;01:02, 10.41s/it, Loss: 2.74392]\rEpoch 2:  69%|██████▉   | 11/16 [01:54&lt;00:51, 10.38s/it, Loss: 2.74392]\rEpoch 2:  69%|██████▉   | 11/16 [01:54&lt;00:51, 10.38s/it, Loss: 2.76289]\rEpoch 2:  75%|███████▌  | 12/16 [02:04&lt;00:41, 10.37s/it, Loss: 2.76289]\rEpoch 2:  75%|███████▌  | 12/16 [02:04&lt;00:41, 10.37s/it, Loss: 2.75963]\rEpoch 2:  81%|████████▏ | 13/16 [02:15&lt;00:31, 10.54s/it, Loss: 2.75963]\rEpoch 2:  81%|████████▏ | 13/16 [02:15&lt;00:31, 10.54s/it, Loss: 2.71548]\rEpoch 2:  88%|████████▊ | 14/16 [02:25&lt;00:20, 10.46s/it, Loss: 2.71548]\rEpoch 2:  88%|████████▊ | 14/16 [02:25&lt;00:20, 10.46s/it, Loss: 2.71429]\rEpoch 2:  94%|█████████▍| 15/16 [02:36&lt;00:10, 10.54s/it, Loss: 2.71429]\rEpoch 2:  94%|█████████▍| 15/16 [02:36&lt;00:10, 10.54s/it, Loss: 2.73832]\rEpoch 2: 100%|██████████| 16/16 [02:39&lt;00:00,  8.32s/it, Loss: 2.73832]\rEpoch 2: 100%|██████████| 16/16 [02:39&lt;00:00,  8.32s/it, Loss: 2.73053]\n2019-09-13 09:25:53,025: ** ** * Saving fine-tuned model ** ** * \n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">2019-09-13 09:17:28,494: device: cpu n_gpu: 0, distributed training: False, 16-bits training: False\n2019-09-13 09:17:29,003: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt not found in cache, downloading to /tmp/tmp1l4zc8ab\n\r  0%|          | 0/213450 [00:00&lt;?, ?B/s]\r  8%|▊         | 17408/213450 [00:00&lt;00:01, 105573.23B/s]\r 24%|██▍       | 52224/213450 [00:00&lt;00:01, 123878.00B/s]\r 49%|████▉     | 104448/213450 [00:00&lt;00:00, 151197.16B/s]\r 82%|████████▏ | 174080/213450 [00:00&lt;00:00, 186890.14B/s]\r100%|██████████| 213450/213450 [00:00&lt;00:00, 318858.23B/s]\n2019-09-13 09:17:30,044: copying /tmp/tmp1l4zc8ab to cache at /root/.cache/torch/pytorch_transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n2019-09-13 09:17:30,045: creating metadata file for /root/.cache/torch/pytorch_transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n2019-09-13 09:17:30,046: removing temp file /tmp/tmp1l4zc8ab\n2019-09-13 09:17:30,046: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /root/.cache/torch/pytorch_transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n2019-09-13 09:17:30,508: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json not found in cache, downloading to /tmp/tmp64tzlt8_\n\r  0%|          | 0/313 [00:00&lt;?, ?B/s]\r100%|██████████| 313/313 [00:00&lt;00:00, 251835.25B/s]\n2019-09-13 09:17:30,903: copying /tmp/tmp64tzlt8_ to cache at /root/.cache/torch/pytorch_transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.d7a3af18ce3a2ab7c0f48f04dc8daff45ed9a3ed333b9e9a79d012a0dedf87a6\n2019-09-13 09:17:30,903: creating metadata file for /root/.cache/torch/pytorch_transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.d7a3af18ce3a2ab7c0f48f04dc8daff45ed9a3ed333b9e9a79d012a0dedf87a6\n2019-09-13 09:17:30,903: removing temp file /tmp/tmp64tzlt8_\n2019-09-13 09:17:30,904: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /root/.cache/torch/pytorch_transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.d7a3af18ce3a2ab7c0f48f04dc8daff45ed9a3ed333b9e9a79d012a0dedf87a6\n2019-09-13 09:17:30,904: Model config {\n  &#34;attention_probs_dropout_prob&#34;: 0.1,\n  &#34;finetuning_task&#34;: null,\n  &#34;hidden_act&#34;: &#34;gelu&#34;,\n  &#34;hidden_dropout_prob&#34;: 0.1,\n  &#34;hidden_size&#34;: 768,\n  &#34;initializer_range&#34;: 0.02,\n  &#34;intermediate_size&#34;: 3072,\n  &#34;layer_norm_eps&#34;: 1e-12,\n  &#34;max_position_embeddings&#34;: 512,\n  &#34;num_attention_heads&#34;: 12,\n  &#34;num_hidden_layers&#34;: 12,\n  &#34;num_labels&#34;: 2,\n  &#34;output_attentions&#34;: false,\n  &#34;output_hidden_states&#34;: false,\n  &#34;torchscript&#34;: false,\n  &#34;type_vocab_size&#34;: 2,\n  &#34;vocab_size&#34;: 28996\n}\n\n2019-09-13 09:17:31,300: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin not found in cache, downloading to /tmp/tmpe8rv61u3\n\r  0%|          | 0/435779157 [00:00&lt;?, ?B/s]\r  0%|          | 52224/435779157 [00:00&lt;22:59, 315795.49B/s]\r  0%|          | 278528/435779157 [00:00&lt;17:41, 410271.85B/s]\r  0%|          | 957440/435779157 [00:00&lt;12:53, 561864.09B/s]\r  1%|          | 2353152/435779157 [00:00&lt;09:09, 789046.61B/s]\r  1%|          | 4104192/435779157 [00:00&lt;06:34, 1095466.28B/s]\r  2%|▏         | 6861824/435779157 [00:00&lt;04:38, 1538752.24B/s]\r  2%|▏         | 9433088/435779157 [00:00&lt;03:18, 2143248.42B/s]\r  3%|▎         | 11591680/435779157 [00:01&lt;02:26, 2897650.70B/s]\r  3%|▎         | 14273536/435779157 [00:01&lt;01:46, 3956297.94B/s]\r  4%|▍         | 16934912/435779157 [00:01&lt;01:18, 5313314.66B/s]\r  4%|▍         | 19119104/435779157 [00:01&lt;01:00, 6859928.93B/s]\r  5%|▌         | 22667264/435779157 [00:01&lt;00:46, 8953528.18B/s]\r  6%|▌         | 26138624/435779157 [00:01&lt;00:35, 11517550.03B/s]\r  7%|▋         | 28897280/435779157 [00:01&lt;00:29, 13663832.32B/s]\r  7%|▋         | 32110592/435779157 [00:01&lt;00:24, 16507012.82B/s]\r  8%|▊         | 34988032/435779157 [00:01&lt;00:21, 18849198.79B/s]\r  9%|▊         | 38122496/435779157 [00:02&lt;00:18, 21409514.88B/s]\r  9%|▉         | 41037824/435779157 [00:02&lt;00:17, 22411876.92B/s]\r 10%|█         | 44226560/435779157 [00:02&lt;00:15, 24604751.49B/s]\r 11%|█         | 47139840/435779157 [00:02&lt;00:15, 25297504.91B/s]\r 12%|█▏        | 50487296/435779157 [00:02&lt;00:14, 25819546.81B/s]\r 12%|█▏        | 53689344/435779157 [00:02&lt;00:13, 27411548.27B/s]\r 13%|█▎        | 56616960/435779157 [00:02&lt;00:13, 27479902.82B/s]\r 14%|█▎        | 59875328/435779157 [00:02&lt;00:13, 28831854.59B/s]\r 14%|█▍        | 62867456/435779157 [00:02&lt;00:13, 27522540.62B/s]\r 15%|█▌        | 65934336/435779157 [00:02&lt;00:13, 28396435.66B/s]\r 16%|█▌        | 68843520/435779157 [00:03&lt;00:12, 28343460.41B/s]\r 17%|█▋        | 71983104/435779157 [00:03&lt;00:13, 27674290.43B/s]\r 17%|█▋        | 75143168/435779157 [00:03&lt;00:12, 28743301.44B/s]\r 18%|█▊        | 78061568/435779157 [00:03&lt;00:12, 28673658.26B/s]\r 19%|█▊        | 81205248/435779157 [00:03&lt;00:12, 29449120.37B/s]\r 19%|█▉        | 84173824/435779157 [00:03&lt;00:12, 27945343.97B/s]\r 20%|██        | 87469056/435779157 [00:03&lt;00:11, 29280029.05B/s]\r 21%|██        | 90436608/435779157 [00:03&lt;00:11, 28791837.44B/s]\r 21%|██▏       | 93577216/435779157 [00:03&lt;00:12, 27888884.18B/s]\r 22%|██▏       | 96824320/435779157 [00:04&lt;00:11, 29121541.81B/s]\r 23%|██▎       | 99770368/435779157 [00:04&lt;00:11, 28702943.48B/s]\r 24%|██▎       | 103172096/435779157 [00:04&lt;00:11, 30113320.10B/s]\r 24%|██▍       | 106219520/435779157 [00:04&lt;00:11, 28420217.95B/s]\r 25%|██▌       | 109305856/435779157 [00:04&lt;00:11, 28446348.86B/s]\r 26%|██▌       | 112278528/435779157 [00:04&lt;00:11, 28817616.63B/s]\r 26%|██▋       | 115204096/435779157 [00:04&lt;00:11, 27921790.06B/s]\r 27%|██▋       | 118458368/435779157 [00:04&lt;00:10, 29164041.35B/s]\r 28%|██▊       | 121405440/435779157 [00:04&lt;00:10, 28692028.92B/s]\r 29%|██▊       | 124657664/435779157 [00:05&lt;00:10, 29738952.23B/s]\r 29%|██▉       | 127657984/435779157 [00:05&lt;00:10, 28121043.77B/s]\r 30%|███       | 130883584/435779157 [00:05&lt;00:10, 28490967.48B/s]\r 31%|███       | 134105088/435779157 [00:05&lt;00:10, 29513305.36B/s]\r 31%|███▏      | 137084928/435779157 [00:05&lt;00:10, 28157785.46B/s]\r 32%|███▏      | 140255232/435779157 [00:05&lt;00:10, 29131911.12B/s]\r 33%|███▎      | 143200256/435779157 [00:05&lt;00:10, 28639428.30B/s]\r 34%|███▎      | 146497536/435779157 [00:05&lt;00:10, 28143942.55B/s]\r 34%|███▍      | 149819392/435779157 [00:05&lt;00:09, 29495344.91B/s]\r 35%|███▌      | 152799232/435779157 [00:05&lt;00:09, 28859319.62B/s]\r 36%|███▌      | 155987968/435779157 [00:06&lt;00:09, 29704773.17B/s]\r 36%|███▋      | 158982144/435779157 [00:06&lt;00:09, 28388630.54B/s]\r 37%|███▋      | 162213888/435779157 [00:06&lt;00:09, 29462549.78B/s]\r 38%|███▊      | 165190656/435779157 [00:06&lt;00:09, 28792607.08B/s]\r 39%|███▊      | 168271872/435779157 [00:06&lt;00:09, 28129839.77B/s]\r 39%|███▉      | 171499520/435779157 [00:06&lt;00:09, 29256021.10B/s]\r 40%|████      | 174451712/435779157 [00:06&lt;00:09, 28704747.34B/s]\r 41%|████      | 177831936/435779157 [00:06&lt;00:08, 30064042.59B/s]\r 42%|████▏     | 180870144/435779157 [00:06&lt;00:08, 28456602.57B/s]\r 42%|████▏     | 184016896/435779157 [00:07&lt;00:08, 28535677.96B/s]\r 43%|████▎     | 187234304/435779157 [00:07&lt;00:08, 29537127.90B/s]\r 44%|████▎     | 190217216/435779157 [00:07&lt;00:08, 28022287.58B/s]\r 44%|████▍     | 193501184/435779157 [00:07&lt;00:08, 29311697.44B/s]\r 45%|████▌     | 196473856/435779157 [00:07&lt;00:08, 28579443.91B/s]\r 46%|████▌     | 199483392/435779157 [00:07&lt;00:08, 27760354.40B/s]\r 47%|████▋     | 202702848/435779157 [00:07&lt;00:08, 28955221.90B/s]\r 47%|████▋     | 205631488/435779157 [00:07&lt;00:08, 28373496.33B/s]\r 48%|████▊     | 208945152/435779157 [00:07&lt;00:07, 29650953.95B/s]\r 49%|████▊     | 211943424/435779157 [00:08&lt;00:07, 28213472.04B/s]\r 49%|████▉     | 215099392/435779157 [00:08&lt;00:07, 29139714.56B/s]\r 50%|█████     | 218047488/435779157 [00:08&lt;00:07, 28870076.14B/s]\r 51%|█████     | 221143040/435779157 [00:08&lt;00:07, 27990858.89B/s]\r 51%|█████▏    | 224292864/435779157 [00:08&lt;00:07, 28957601.92B/s]\r 52%|█████▏    | 227213312/435779157 [00:08&lt;00:07, 28475013.21B/s]\r 53%|█████▎    | 230514688/435779157 [00:08&lt;00:06, 29699454.19B/s]\r 54%|█████▎    | 233511936/435779157 [00:08&lt;00:07, 28049681.68B/s]\r 54%|█████▍    | 236724224/435779157 [00:08&lt;00:07, 28361428.38B/s]\r 55%|█████▌    | 239837184/435779157 [00:09&lt;00:06, 29136749.06B/s]\r 56%|█████▌    | 242776064/435779157 [00:09&lt;00:06, 27960027.94B/s]\r 56%|█████▋    | 246210560/435779157 [00:09&lt;00:06, 29611266.54B/s]\r 57%|█████▋    | 249218048/435779157 [00:09&lt;00:06, 28864609.10B/s]\r 58%|█████▊    | 252361728/435779157 [00:09&lt;00:06, 29590288.39B/s]\r 59%|█████▊    | 255350784/435779157 [00:09&lt;00:06, 28276704.17B/s]\r 59%|█████▉    | 258433024/435779157 [00:09&lt;00:06, 28419473.67B/s]\r 60%|██████    | 261579776/435779157 [00:09&lt;00:05, 29269512.80B/s]\r 61%|██████    | 264529920/435779157 [00:09&lt;00:06, 28218278.71B/s]\r 61%|██████▏   | 267907072/435779157 [00:09&lt;00:05, 29681967.85B/s]\r 62%|██████▏   | 270912512/435779157 [00:10&lt;00:05, 28900680.87B/s]\r 63%|██████▎   | 273955840/435779157 [00:10&lt;00:05, 29343991.91B/s]\r 64%|██████▎   | 276914176/435779157 [00:10&lt;00:05, 28415191.59B/s]\r 64%|██████▍   | 280289280/435779157 [00:10&lt;00:05, 28757125.75B/s]\r 65%|██████▌   | 283557888/435779157 [00:10&lt;00:05, 29831647.42B/s]\r 66%|██████▌   | 286564352/435779157 [00:10&lt;00:05, 28843832.54B/s]\r 66%|██████▋   | 289759232/435779157 [00:10&lt;00:04, 29707173.47B/s]\r 67%|██████▋   | 292752384/435779157 [00:10&lt;00:04, 28869866.21B/s]\r 68%|██████▊   | 295991296/435779157 [00:10&lt;00:04, 29842092.74B/s]\r 69%|██████▊   | 298998784/435779157 [00:11&lt;00:04, 28622469.59B/s]\r 69%|██████▉   | 302227456/435779157 [00:11&lt;00:04, 28728522.62B/s]\r 70%|███████   | 305463296/435779157 [00:11&lt;00:04, 29728888.68B/s]\r 71%|███████   | 308458496/435779157 [00:11&lt;00:04, 28678496.41B/s]\r 72%|███████▏  | 311633920/435779157 [00:11&lt;00:04, 29535986.26B/s]\r 72%|███████▏  | 314610688/435779157 [00:11&lt;00:04, 28716586.54B/s]\r 73%|███████▎  | 317988864/435779157 [00:11&lt;00:03, 30065108.51B/s]\r 74%|███████▎  | 321027072/435779157 [00:11&lt;00:03, 28867403.43B/s]\r 74%|███████▍  | 324116480/435779157 [00:11&lt;00:03, 28596606.06B/s]\r 75%|███████▌  | 327314432/435779157 [00:12&lt;00:03, 29533484.77B/s]\r 76%|███████▌  | 330375168/435779157 [00:12&lt;00:03, 28747032.83B/s]\r 77%|███████▋  | 333547520/435779157 [00:12&lt;00:03, 29578753.56B/s]\r 77%|███████▋  | 336526336/435779157 [00:12&lt;00:03, 28661618.20B/s]\r 78%|███████▊  | 339872768/435779157 [00:12&lt;00:03, 29950659.59B/s]\r 79%|███████▊  | 342897664/435779157 [00:12&lt;00:03, 28801414.84B/s]\r 79%|███████▉  | 345972736/435779157 [00:12&lt;00:03, 28538400.79B/s]\r 80%|████████  | 349150208/435779157 [00:12&lt;00:02, 29437667.12B/s]\r 81%|████████  | 352165888/435779157 [00:12&lt;00:02, 28537186.00B/s]\r 82%|████████▏ | 355340288/435779157 [00:12&lt;00:02, 29428261.49B/s]\r 82%|████████▏ | 358305792/435779157 [00:13&lt;00:02, 28671772.39B/s]\r 83%|████████▎ | 361553920/435779157 [00:13&lt;00:02, 29716840.15B/s]\r 84%|████████▎ | 364550144/435779157 [00:13&lt;00:02, 28637603.01B/s]\r 84%|████████▍ | 367812608/435779157 [00:13&lt;00:02, 28725996.82B/s]\r 85%|████████▌ | 371021824/435779157 [00:13&lt;00:02, 29659023.65B/s]\r 86%|████████▌ | 374022144/435779157 [00:13&lt;00:02, 28641700.76B/s]\r 87%|████████▋ | 377267200/435779157 [00:13&lt;00:01, 29686765.43B/s]\r 87%|████████▋ | 380261376/435779157 [00:13&lt;00:01, 29078860.62B/s]\r 88%|████████▊ | 383404032/435779157 [00:13&lt;00:01, 29744819.00B/s]\r 89%|████████▊ | 386397184/435779157 [00:14&lt;00:01, 28425263.46B/s]\r 89%|████████▉ | 389472256/435779157 [00:14&lt;00:01, 29083389.49B/s]\r 90%|█████████ | 392401920/435779157 [00:14&lt;00:01, 28535511.65B/s]\r 91%|█████████ | 395747328/435779157 [00:14&lt;00:01, 28496536.24B/s]\r 92%|█████████▏| 399072256/435779157 [00:14&lt;00:01, 29773154.58B/s]\r 92%|█████████▏| 402073600/435779157 [00:14&lt;00:01, 28864755.49B/s]\r 93%|█████████▎| 405512192/435779157 [00:14&lt;00:01, 28656392.08B/s]\r 94%|█████████▍| 408914944/435779157 [00:14&lt;00:00, 30080257.43B/s]\r 95%|█████████▍| 411953152/435779157 [00:14&lt;00:00, 29166430.85B/s]\r 95%|█████████▌| 415149056/435779157 [00:15&lt;00:00, 29950694.34B/s]\r 96%|█████████▌| 418168832/435779157 [00:15&lt;00:00, 28853098.06B/s]\r 97%|█████████▋| 421339136/435779157 [00:15&lt;00:00, 28790713.20B/s]\r 97%|█████████▋| 424400896/435779157 [00:15&lt;00:00, 29314474.91B/s]\r 98%|█████████▊| 427433984/435779157 [00:15&lt;00:00, 28506446.08B/s]\r 99%|█████████▉| 430580736/435779157 [00:15&lt;00:00, 29333897.88B/s]\r 99%|█████████▉| 433530880/435779157 [00:15&lt;00:00, 28677995.43B/s]\r100%|██████████| 435779157/435779157 [00:15&lt;00:00, 27718892.14B/s]\n2019-09-13 09:17:47,488: copying /tmp/tmpe8rv61u3 to cache at /root/.cache/torch/pytorch_transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\n2019-09-13 09:17:48,057: creating metadata file for /root/.cache/torch/pytorch_transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\n2019-09-13 09:17:48,057: removing temp file /tmp/tmpe8rv61u3\n2019-09-13 09:17:48,136: loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /root/.cache/torch/pytorch_transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\n2019-09-13 09:17:52,567: ***** Running training *****\n2019-09-13 09:17:52,567:   Num examples = 1468\n2019-09-13 09:17:52,568:   Batch size = 32\n2019-09-13 09:17:52,568:   Num steps = 45\n2019-09-13 09:17:52,570: Loading training examples for epoch 0\n\rTraining examples:   0%|          | 0/490 [00:00&lt;?, ?it/s]\rTraining examples: 100%|██████████| 490/490 [00:00&lt;00:00, 7828.17it/s]\n2019-09-13 09:17:52,633: Loading complete!\n\rEpoch 0:   0%|          | 0/16 [00:00&lt;?, ?it/s]\rEpoch 0:   6%|▋         | 1/16 [00:10&lt;02:34, 10.32s/it]\rEpoch 0:   6%|▋         | 1/16 [00:10&lt;02:34, 10.32s/it, Loss: 6.93127]\rEpoch 0:  12%|█▎        | 2/16 [00:21&lt;02:26, 10.43s/it, Loss: 6.93127]\rEpoch 0:  12%|█▎        | 2/16 [00:21&lt;02:26, 10.43s/it, Loss: 6.02028]\rEpoch 0:  19%|█▉        | 3/16 [00:31&lt;02:16, 10.48s/it, Loss: 6.02028]\rEpoch 0:  19%|█▉        | 3/16 [00:31&lt;02:16, 10.48s/it, Loss: 5.55043]\rEpoch 0:  25%|██▌       | 4/16 [00:42&lt;02:06, 10.52s/it, Loss: 5.55043]\rEpoch 0:  25%|██▌       | 4/16 [00:42&lt;02:06, 10.52s/it, Loss: 5.17909]\rEpoch 0:  31%|███▏      | 5/16 [00:52&lt;01:55, 10.47s/it, Loss: 5.17909]\rEpoch 0:  31%|███▏      | 5/16 [00:52&lt;01:55, 10.47s/it, Loss: 4.78345]\rEpoch 0:  38%|███▊      | 6/16 [01:03&lt;01:45, 10.52s/it, Loss: 4.78345]\rEpoch 0:  38%|███▊      | 6/16 [01:03&lt;01:45, 10.52s/it, Loss: 4.58648]\rEpoch 0:  44%|████▍     | 7/16 [01:13&lt;01:34, 10.45s/it, Loss: 4.58648]\rEpoch 0:  44%|████▍     | 7/16 [01:13&lt;01:34, 10.45s/it, Loss: 4.41470]\rEpoch 0:  50%|█████     | 8/16 [01:23&lt;01:23, 10.42s/it, Loss: 4.41470]\rEpoch 0:  50%|█████     | 8/16 [01:23&lt;01:23, 10.42s/it, Loss: 4.25623]\rEpoch 0:  56%|█████▋    | 9/16 [01:34&lt;01:12, 10.39s/it, Loss: 4.25623]\rEpoch 0:  56%|█████▋    | 9/16 [01:34&lt;01:12, 10.39s/it, Loss: 4.17582]\rEpoch 0:  62%|██████▎   | 10/16 [01:44&lt;01:02, 10.37s/it, Loss: 4.17582]\rEpoch 0:  62%|██████▎   | 10/16 [01:44&lt;01:02, 10.37s/it, Loss: 4.07387]\rEpoch 0:  69%|██████▉   | 11/16 [01:54&lt;00:51, 10.37s/it, Loss: 4.07387]\rEpoch 0:  69%|██████▉   | 11/16 [01:54&lt;00:51, 10.37s/it, Loss: 4.00849]\rEpoch 0:  75%|███████▌  | 12/16 [02:05&lt;00:41, 10.36s/it, Loss: 4.00849]\rEpoch 0:  75%|███████▌  | 12/16 [02:05&lt;00:41, 10.36s/it, Loss: 3.95066]\rEpoch 0:  81%|████████▏ | 13/16 [02:15&lt;00:31, 10.34s/it, Loss: 3.95066]\rEpoch 0:  81%|████████▏ | 13/16 [02:15&lt;00:31, 10.34s/it, Loss: 3.92277]\rEpoch 0:  88%|████████▊ | 14/16 [02:25&lt;00:20, 10.38s/it, Loss: 3.92277]\rEpoch 0:  88%|████████▊ | 14/16 [02:25&lt;00:20, 10.38s/it, Loss: 3.85591]\rEpoch 0:  94%|█████████▍| 15/16 [02:36&lt;00:10, 10.48s/it, Loss: 3.85591]\rEpoch 0:  94%|█████████▍| 15/16 [02:36&lt;00:10, 10.48s/it, Loss: 3.81644]\rEpoch 0: 100%|██████████| 16/16 [02:40&lt;00:00,  8.35s/it, Loss: 3.81644]\rEpoch 0: 100%|██████████| 16/16 [02:40&lt;00:00,  8.35s/it, Loss: 3.81515]\n2019-09-13 09:20:33,074: Loading training examples for epoch 1\n\rTraining examples:   0%|          | 0/489 [00:00&lt;?, ?it/s]\rTraining examples: 100%|██████████| 489/489 [00:00&lt;00:00, 7557.64it/s]\n2019-09-13 09:20:33,140: Loading complete!\n\rEpoch 1:   0%|          | 0/16 [00:00&lt;?, ?it/s]\rEpoch 1:   6%|▋         | 1/16 [00:10&lt;02:30, 10.00s/it]\rEpoch 1:   6%|▋         | 1/16 [00:10&lt;02:30, 10.00s/it, Loss: 3.32886]\rEpoch 1:  12%|█▎        | 2/16 [00:20&lt;02:22, 10.21s/it, Loss: 3.32886]\rEpoch 1:  12%|█▎        | 2/16 [00:20&lt;02:22, 10.21s/it, Loss: 3.27360]\rEpoch 1:  19%|█▉        | 3/16 [00:31&lt;02:13, 10.27s/it, Loss: 3.27360]\rEpoch 1:  19%|█▉        | 3/16 [00:31&lt;02:13, 10.27s/it, Loss: 3.03251]\rEpoch 1:  25%|██▌       | 4/16 [00:41&lt;02:03, 10.30s/it, Loss: 3.03251]\rEpoch 1:  25%|██▌       | 4/16 [00:41&lt;02:03, 10.30s/it, Loss: 2.94789]\rEpoch 1:  31%|███▏      | 5/16 [00:52&lt;01:54, 10.37s/it, Loss: 2.94789]\rEpoch 1:  31%|███▏      | 5/16 [00:52&lt;01:54, 10.37s/it, Loss: 2.92848]\rEpoch 1:  38%|███▊      | 6/16 [01:02&lt;01:43, 10.35s/it, Loss: 2.92848]\rEpoch 1:  38%|███▊      | 6/16 [01:02&lt;01:43, 10.35s/it, Loss: 2.94156]\rEpoch 1:  44%|████▍     | 7/16 [01:12&lt;01:33, 10.42s/it, Loss: 2.94156]\rEpoch 1:  44%|████▍     | 7/16 [01:12&lt;01:33, 10.42s/it, Loss: 2.94744]\rEpoch 1:  50%|█████     | 8/16 [01:23&lt;01:23, 10.39s/it, Loss: 2.94744]\rEpoch 1:  50%|█████     | 8/16 [01:23&lt;01:23, 10.39s/it, Loss: 2.92721]\rEpoch 1:  56%|█████▋    | 9/16 [01:33&lt;01:12, 10.37s/it, Loss: 2.92721]\rEpoch 1:  56%|█████▋    | 9/16 [01:33&lt;01:12, 10.37s/it, Loss: 2.90332]\rEpoch 1:  62%|██████▎   | 10/16 [01:43&lt;01:02, 10.39s/it, Loss: 2.90332]\rEpoch 1:  62%|██████▎   | 10/16 [01:43&lt;01:02, 10.39s/it, Loss: 2.92808]\rEpoch 1:  69%|██████▉   | 11/16 [01:54&lt;00:52, 10.42s/it, Loss: 2.92808]\rEpoch 1:  69%|██████▉   | 11/16 [01:54&lt;00:52, 10.42s/it, Loss: 2.93523]\rEpoch 1:  75%|███████▌  | 12/16 [02:04&lt;00:41, 10.42s/it, Loss: 2.93523]\rEpoch 1:  75%|███████▌  | 12/16 [02:04&lt;00:41, 10.42s/it, Loss: 2.92545]\rEpoch 1:  81%|████████▏ | 13/16 [02:15&lt;00:31, 10.38s/it, Loss: 2.92545]\rEpoch 1:  81%|████████▏ | 13/16 [02:15&lt;00:31, 10.38s/it, Loss: 2.93394]\rEpoch 1:  88%|████████▊ | 14/16 [02:25&lt;00:20, 10.41s/it, Loss: 2.93394]\rEpoch 1:  88%|████████▊ | 14/16 [02:25&lt;00:20, 10.41s/it, Loss: 2.92805]\rEpoch 1:  94%|█████████▍| 15/16 [02:36&lt;00:10, 10.43s/it, Loss: 2.92805]\rEpoch 1:  94%|█████████▍| 15/16 [02:36&lt;00:10, 10.43s/it, Loss: 2.93455]\rEpoch 1: 100%|██████████| 16/16 [02:39&lt;00:00,  8.23s/it, Loss: 2.93455]\rEpoch 1: 100%|██████████| 16/16 [02:39&lt;00:00,  8.23s/it, Loss: 2.97023]\n2019-09-13 09:23:12,800: Loading training examples for epoch 2\n\rTraining examples:   0%|          | 0/489 [00:00&lt;?, ?it/s]\rTraining examples: 100%|██████████| 489/489 [00:00&lt;00:00, 7998.25it/s]\n2019-09-13 09:23:12,862: Loading complete!\n\rEpoch 2:   0%|          | 0/16 [00:00&lt;?, ?it/s]\rEpoch 2:   6%|▋         | 1/16 [00:10&lt;02:34, 10.30s/it]\rEpoch 2:   6%|▋         | 1/16 [00:10&lt;02:34, 10.30s/it, Loss: 2.56884]\rEpoch 2:  12%|█▎        | 2/16 [00:20&lt;02:24, 10.30s/it, Loss: 2.56884]\rEpoch 2:  12%|█▎        | 2/16 [00:20&lt;02:24, 10.30s/it, Loss: 2.49181]\rEpoch 2:  19%|█▉        | 3/16 [00:30&lt;02:14, 10.31s/it, Loss: 2.49181]\rEpoch 2:  19%|█▉        | 3/16 [00:30&lt;02:14, 10.31s/it, Loss: 2.40388]\rEpoch 2:  25%|██▌       | 4/16 [00:41&lt;02:04, 10.39s/it, Loss: 2.40388]\rEpoch 2:  25%|██▌       | 4/16 [00:41&lt;02:04, 10.39s/it, Loss: 2.50817]\rEpoch 2:  31%|███▏      | 5/16 [00:51&lt;01:54, 10.40s/it, Loss: 2.50817]\rEpoch 2:  31%|███▏      | 5/16 [00:51&lt;01:54, 10.40s/it, Loss: 2.58843]\rEpoch 2:  38%|███▊      | 6/16 [01:02&lt;01:43, 10.40s/it, Loss: 2.58843]\rEpoch 2:  38%|███▊      | 6/16 [01:02&lt;01:43, 10.40s/it, Loss: 2.60762]\rEpoch 2:  44%|████▍     | 7/16 [01:12&lt;01:33, 10.41s/it, Loss: 2.60762]\rEpoch 2:  44%|████▍     | 7/16 [01:12&lt;01:33, 10.41s/it, Loss: 2.66570]\rEpoch 2:  50%|█████     | 8/16 [01:23&lt;01:23, 10.39s/it, Loss: 2.66570]\rEpoch 2:  50%|█████     | 8/16 [01:23&lt;01:23, 10.39s/it, Loss: 2.69110]\rEpoch 2:  56%|█████▋    | 9/16 [01:33&lt;01:13, 10.44s/it, Loss: 2.69110]\rEpoch 2:  56%|█████▋    | 9/16 [01:33&lt;01:13, 10.44s/it, Loss: 2.70089]\rEpoch 2:  62%|██████▎   | 10/16 [01:44&lt;01:02, 10.41s/it, Loss: 2.70089]\rEpoch 2:  62%|██████▎   | 10/16 [01:44&lt;01:02, 10.41s/it, Loss: 2.74392]\rEpoch 2:  69%|██████▉   | 11/16 [01:54&lt;00:51, 10.38s/it, Loss: 2.74392]\rEpoch 2:  69%|██████▉   | 11/16 [01:54&lt;00:51, 10.38s/it, Loss: 2.76289]\rEpoch 2:  75%|███████▌  | 12/16 [02:04&lt;00:41, 10.37s/it, Loss: 2.76289]\rEpoch 2:  75%|███████▌  | 12/16 [02:04&lt;00:41, 10.37s/it, Loss: 2.75963]\rEpoch 2:  81%|████████▏ | 13/16 [02:15&lt;00:31, 10.54s/it, Loss: 2.75963]\rEpoch 2:  81%|████████▏ | 13/16 [02:15&lt;00:31, 10.54s/it, Loss: 2.71548]\rEpoch 2:  88%|████████▊ | 14/16 [02:25&lt;00:20, 10.46s/it, Loss: 2.71548]\rEpoch 2:  88%|████████▊ | 14/16 [02:25&lt;00:20, 10.46s/it, Loss: 2.71429]\rEpoch 2:  94%|█████████▍| 15/16 [02:36&lt;00:10, 10.54s/it, Loss: 2.71429]\rEpoch 2:  94%|█████████▍| 15/16 [02:36&lt;00:10, 10.54s/it, Loss: 2.73832]\rEpoch 2: 100%|██████████| 16/16 [02:39&lt;00:00,  8.32s/it, Loss: 2.73832]\rEpoch 2: 100%|██████████| 16/16 [02:39&lt;00:00,  8.32s/it, Loss: 2.73053]\n2019-09-13 09:25:53,025: ** ** * Saving fine-tuned model ** ** * \n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%sh\necho hello\nls -la /home/\nls -la /home/finetune_lm/\nls -la /home/finetune_lm/model_eng/"],"metadata":{"application/vnd.home.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2548cbc9-6605-4d08-879a-7765e8c82c6a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.home.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">hello\ntotal 4\ndrwxrwxrwx 1 root root 4096 Jan  1  1970 .\ndrwxr-xr-x 1 root root 4096 Sep 13 07:30 ..\ndrwxr-xr-x 1 root root    0 May 31 13:58 ##########\ndrwxr-xr-x 1 root root    0 Apr 24 15:57 C\ndrwxr-xr-x 1 root root    0 Jan  1  1970 home-datasets\ndrwxr-xr-x 1 root root    0 Jun 12 15:07 home-results\ndrwxr-xr-x 1 root root    0 Jun 21 13:37 home\ndrwxr-xr-x 1 root root    0 Apr 24 15:56 file\ndrwxr-xr-x 1 root root    0 Mar 13  2019 FileStore\ndrwxr-xr-x 1 root root    0 Sep 13 08:33 finetune_lm\ndrwxr-xr-x 1 root root    0 Apr 24 14:00 list_errors.csv\ndrwxr-xr-x 1 root root    0 Apr 24 14:54 list_errors_robin.csv\ndrwxrwx--- 2 root root 4096 Jan  1  1970 ml\ndrwxr-xr-x 1 root root    0 Mar 11  2019 mnt\ndrwxr-xr-x 1 root root    0 Apr 17 11:45 mycsv.csv\ndrwxr-xr-x 1 root root    0 Jun 21 14:38 TestGPA\ndrwxr-xr-x 1 root root    0 Mar  4  2019 tmp\ndrwxr-xr-x 1 root root    0 Apr 16 08:37 user\ndrwxr-xr-x 1 root root    0 Aug  7 07:33 vgadlvceaadevsa01.blob.core.windows.net\ntotal 0\ndrwxr-xr-x 1 root root    0 Sep 13 08:33 .\ndrwxrwxrwx 1 root root 4096 Jan  1  1970 ..\ndrwxr-xr-x 1 root root    0 Sep 13 09:27 model_eng\ntotal 0\ndrwxr-xr-x 1 root root         0 Sep 13 09:27 .\ndrwxr-xr-x 1 root root         0 Sep 13 08:33 ..\n-rw-r--r-- 1 root root         2 Sep 13 09:27 added_tokens.json\n-rw-r--r-- 1 root root       473 Sep 13 09:25 config.json\n-rw-r--r-- 1 root root 435780426 Sep 13 09:27 pytorch_model.bin\n-rw-r--r-- 1 root root       112 Sep 13 09:27 special_tokens_map.json\n-rw-r--r-- 1 root root    213450 Sep 13 09:27 vocab.txt\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">hello\ntotal 4\ndrwxrwxrwx 1 root root 4096 Jan  1  1970 .\ndrwxr-xr-x 1 root root 4096 Sep 13 07:30 ..\ndrwxr-xr-x 1 root root    0 May 31 13:58 ##########\ndrwxr-xr-x 1 root root    0 Apr 24 15:57 C\ndrwxr-xr-x 1 root root    0 Jan  1  1970 home-datasets\ndrwxr-xr-x 1 root root    0 Jun 12 15:07 home-results\ndrwxr-xr-x 1 root root    0 Jun 21 13:37 home\ndrwxr-xr-x 1 root root    0 Apr 24 15:56 file\ndrwxr-xr-x 1 root root    0 Mar 13  2019 FileStore\ndrwxr-xr-x 1 root root    0 Sep 13 08:33 finetune_lm\ndrwxr-xr-x 1 root root    0 Apr 24 14:00 list_errors.csv\ndrwxr-xr-x 1 root root    0 Apr 24 14:54 list_errors_robin.csv\ndrwxrwx--- 2 root root 4096 Jan  1  1970 ml\ndrwxr-xr-x 1 root root    0 Mar 11  2019 mnt\ndrwxr-xr-x 1 root root    0 Apr 17 11:45 mycsv.csv\ndrwxr-xr-x 1 root root    0 Jun 21 14:38 TestGPA\ndrwxr-xr-x 1 root root    0 Mar  4  2019 tmp\ndrwxr-xr-x 1 root root    0 Apr 16 08:37 user\ndrwxr-xr-x 1 root root    0 Aug  7 07:33 vgadlvceaadevsa01.blob.core.windows.net\ntotal 0\ndrwxr-xr-x 1 root root    0 Sep 13 08:33 .\ndrwxrwxrwx 1 root root 4096 Jan  1  1970 ..\ndrwxr-xr-x 1 root root    0 Sep 13 09:27 model_eng\ntotal 0\ndrwxr-xr-x 1 root root         0 Sep 13 09:27 .\ndrwxr-xr-x 1 root root         0 Sep 13 08:33 ..\n-rw-r--r-- 1 root root         2 Sep 13 09:27 added_tokens.json\n-rw-r--r-- 1 root root       473 Sep 13 09:25 config.json\n-rw-r--r-- 1 root root 435780426 Sep 13 09:27 pytorch_model.bin\n-rw-r--r-- 1 root root       112 Sep 13 09:27 special_tokens_map.json\n-rw-r--r-- 1 root root    213450 Sep 13 09:27 vocab.txt\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%sh\nls .\nls -la /home/finetune_lm/model/\nzip -r /home/finetune_lm/bert_finetuned_model.zip /home/finetune_lm/.\necho\nls -la /home/finetune_lm/"],"metadata":{"application/vnd.home.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bf8a8ae6-852d-4a7b-a442-e1d3ecac1a70"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sh\ncd /home/finetune_lm\nls -la .\nunzip -t bert_finetuned_model.zip"],"metadata":{"application/vnd.home.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"64fa2578-3ecd-446b-83fe-3ee784149e5d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.cp(\"home:/finetune_lm/model/pytorch_model.bin\", \"home:/finetune_lm/model/\")"],"metadata":{"application/vnd.home.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6666b418-ade0-4e8d-a3a0-b99b44adfbdb"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.home.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3a772a22-bb7f-4ac4-93b5-20fb1e020dc8"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.home.v1+notebook":{"notebookName":"finetune_on_pregenerated","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2798017980702475}},"nbformat":4,"nbformat_minor":0}
